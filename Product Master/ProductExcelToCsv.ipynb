{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd841ba-4fe0-484b-b9c2-8b9a1fb406f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4a251d-337f-463f-ae97-5e7e36770101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from zoneinfo import ZoneInfo\n",
    "from pyspark.sql.functions import current_timestamp, from_utc_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd375ec0-a593-4084-8898-aeffc2077e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1️⃣ Setup parametseters\n",
    "# =========================\n",
    "dbutils.widgets.text(\"master_data_type\", \"product_master\")  # product_master or product_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba4684b-e38e-4fc5-bba7-9f95801b31df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Runtime config (same pattern as DLT)\n",
    "# -------------------------------------------------\n",
    "CATALOG = spark.conf.get(\"rsmas.catalog\")\n",
    "SCHEMA = spark.conf.get(\"rsmas.schema\")\n",
    "SECRET_SCOPE = spark.conf.get(\"rsmas.secret.scope\")\n",
    "\n",
    "BASE_PATH = dbutils.secrets.get(\n",
    "    scope=SECRET_SCOPE,\n",
    "    key=\"master-data-path\"\n",
    ")\n",
    "\n",
    "dataset_type = dbutils.widgets.get(\"master_data_type\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Define paths per dataset\n",
    "# -------------------------------------------------\n",
    "if dataset_type == \"product_master\":\n",
    "    source_path = BASE_PATH + \"product/product_excel/\"\n",
    "    staging_path = BASE_PATH + \"product/product_updates/\"\n",
    "    unique_keys = [\"ProductID\"]\n",
    "\n",
    "elif dataset_type == \"product_price_history\":\n",
    "    source_path = BASE_PATH + \"product/product_price_history_excel/\"\n",
    "    staging_path = BASE_PATH + \"product/product_price_updates/\"\n",
    "    unique_keys = [\"ProductID\", \"StoreID\"]\n",
    "\n",
    "elif dataset_type == \"supplier_costprice_history\":\n",
    "    source_path = BASE_PATH + \"product/supplier_costprice_history_excel/\"\n",
    "    staging_path = BASE_PATH + \"product/supplier_costprice_updates/\"\n",
    "    unique_keys = [\"ProductID\"]\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset_type: {dataset_type}\")\n",
    "\n",
    "checkpoint_path = source_path + \"checkpoints/\"\n",
    "schema_location = checkpoint_path + \"schema/\"\n",
    "\n",
    "# =================================================\n",
    "# Read Excel files as binary\n",
    "# =================================================\n",
    "raw_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .load(source_path)\n",
    "    .filter(\"path NOT LIKE '%checkpoint%'\")\n",
    "    .select(\"path\", \"content\")\n",
    ")\n",
    "\n",
    "# =================================================\n",
    "# ForeachBatch logic\n",
    "# =================================================\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    pdf = batch_df.toPandas()\n",
    "    all_records = []\n",
    "\n",
    "    for _, row in pdf.iterrows():\n",
    "        file_path = row[\"path\"]\n",
    "        content = row[\"content\"]\n",
    "\n",
    "        excel_sheets = pd.read_excel(io.BytesIO(content), sheet_name=None)\n",
    "\n",
    "        for sheet_name, sheet_df in excel_sheets.items():\n",
    "            sheet_df.columns = (\n",
    "                sheet_df.columns.str.strip()\n",
    "                .str.replace(\" \", \"_\", regex=True)\n",
    "                .str.replace(r\"[;{}()\\n\\t=]\", \"\", regex=True)\n",
    "            )\n",
    "\n",
    "            sheet_df[\"source_file\"] = file_path\n",
    "            sheet_df[\"sheet_name\"] = sheet_name\n",
    "            sheet_df[\"load_date\"] = datetime.now(\n",
    "                ZoneInfo(\"Europe/London\")\n",
    "            ).date()\n",
    "\n",
    "            all_records.append(sheet_df)\n",
    "\n",
    "    if all_records:\n",
    "        final_pdf = pd.concat(all_records, ignore_index=True)\n",
    "        spark_df = spark.createDataFrame(final_pdf)\n",
    "\n",
    "        spark_df = spark_df.withColumn(\n",
    "            \"ingested_at\",\n",
    "            from_utc_timestamp(current_timestamp(), \"Europe/London\")\n",
    "        )\n",
    "\n",
    "        spark_df = spark_df.drop_duplicates(subset=unique_keys)\n",
    "\n",
    "        (\n",
    "            spark_df.write\n",
    "            .format(\"delta\")\n",
    "            .partitionBy(\"load_date\")\n",
    "            .mode(\"append\")\n",
    "            .save(staging_path)\n",
    "        )\n",
    "\n",
    "# =================================================\n",
    "# Start stream\n",
    "# =================================================\n",
    "query = (\n",
    "    raw_df.writeStream\n",
    "    .foreachBatch(process_batch)\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30eeab87-0957-4ee4-9c5a-49e6f0c3c398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ProductExcelToCsv",
   "widgets": {
    "master_data_type": {
     "currentValue": "product_master",
     "nuid": "8a14ff3c-1788-4ceb-a915-cac264f44df3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "product_master",
      "label": null,
      "name": "master_data_type",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "product_master",
      "label": null,
      "name": "master_data_type",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
