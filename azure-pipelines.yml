trigger:
- main

pool:
  name: Default
  demands:
    - agent.name -equals RSMAS-Agent

variables:
- group: test-dlt-vars  # Databricks credentials & target catalog/schema

stages:

# =====================================================
# Stage 1️⃣ Build
# =====================================================
- stage: Build
  displayName: "Build DLT Artifact"
  jobs:
  - job: BuildJob
    displayName: "Build DLT Artifact"
    pool:
      name: Default
      demands:
        - agent.name -equals RSMAS-Agent
    steps:

    - script: |
        python --version
        pip --version
      displayName: "Verify Python"

    - script: |
        echo "✅ Dependencies installed"
      displayName: "Install dependencies"

    - publish: .
      artifact: DLT_Artifact
      displayName: "Publish DLT Artifact"

# =====================================================
# Stage 2️⃣ Deploy
# =====================================================
- stage: Deploy
  displayName: "Deploy All DLT Pipelines & Notebooks"
  dependsOn: Build
  jobs:
  - job: DeployJob
    displayName: "Deploy DLT Pipelines & Notebooks"
    pool:
      name: Default
      demands:
        - agent.name -equals RSMAS-Agent
    steps:

    - download: current
      artifact: DLT_Artifact
      displayName: "Download DLT Artifact"

    # 1️⃣ Deploy all DLT pipeline JSONs
    - task: Bash@3
      displayName: "Deploy all DLT Pipelines"
      inputs:
        targetType: 'inline'
        script: |
          echo "Deploying DLT pipelines..."
          for PIPELINE_FILE in ./pipelines/*.json; do
            echo "Updating pipeline JSON: $PIPELINE_FILE"
            python ./scripts/update_pipeline_json.py \
              --file "$PIPELINE_FILE" \
              --catalog $(DLT_CATALOG) \
              --schema $(DLT_SCHEMA) \
              --secret_scope $(DLT_SECRET_SCOPE)

            PIPELINE_JSON=$(jq -c . "$PIPELINE_FILE")
            PIPELINE_NAME=$(jq -r .name "$PIPELINE_FILE")

            # Try to create the pipeline
            RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST \
              -H "Authorization: Bearer $(DATABRICKS_TOKEN)" \
              -H "Content-Type: application/json" \
              -d "$PIPELINE_JSON" \
              https://$(DATABRICKS_HOST)/api/2.0/pipelines)

            if [ "$RESPONSE" == "409" ]; then
              echo "Pipeline '$PIPELINE_NAME' exists, updating..."
              PIPELINE_ID=$(curl -s -H "Authorization: Bearer $(DATABRICKS_TOKEN)" \
                https://$(DATABRICKS_HOST)/api/2.0/pipelines | \
                jq -r --arg NAME "$PIPELINE_NAME" '.pipelines[] | select(.name==$NAME) | .pipeline_id')

              curl -s -X PATCH \
                -H "Authorization: Bearer $(DATABRICKS_TOKEN)" \
                -H "Content-Type: application/json" \
                -d "$PIPELINE_JSON" \
                https://$(DATABRICKS_HOST)/api/2.0/pipelines/$PIPELINE_ID
            fi
          done

    # 2️⃣ Deploy all notebooks
    - task: Bash@3
      displayName: "Deploy all Notebooks"
      inputs:
        targetType: 'inline'
        script: |
          echo "Deploying notebooks..."
          find ./ -name "*.ipynb" | while read NOTEBOOK; do
            REL_PATH=${NOTEBOOK#./}
            TARGET_PATH="/Repos/RSMAS_ADB/dlt/$REL_PATH"
            echo "Deploying $NOTEBOOK -> $TARGET_PATH"

            CONTENT=$(base64 -w 0 "$NOTEBOOK" || base64 "$NOTEBOOK" | tr -d '\n')

            curl -s -X POST \
              -H "Authorization: Bearer $(DATABRICKS_TOKEN)" \
              -H "Content-Type: application/json" \
              -d '{
                "path": "'"$TARGET_PATH"'",
                "language": "PYTHON",
                "overwrite": true,
                "content": "'"$CONTENT"'"
              }' \
              https://$(DATABRICKS_HOST)/api/2.0/workspace/import
          done
